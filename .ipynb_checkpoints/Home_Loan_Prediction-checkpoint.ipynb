{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Loan Prediction\n",
    "This dataset `full_home_loans.csv` is about home loan applications in Washington state, USA, where each row of the dataset is an individual loan application. Your goal in this assignment is to build a machine learning model that can accurately predict whether a given loan application was accepted or rejected. \n",
    "\n",
    "\n",
    "## Part 1: Data Exploration\n",
    "The first few exercises will get you used to looking at the data using `pandas`. Pandas is a widely used library in python for manipulating data. Why? Datasets can consume a _lot_ of space in your computer's memory and traditional python data structures like lists or dictionaries will become painfully slow as we add thousands of rows of data. We use a specialized dataset library `pandas` which has a specialized data structure called a `dataframe` designed to be ultra fast & efficient. Documentation is here: https://pandas.pydata.org/pandas-docs/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas # import pandas library\n",
    "df = pandas.read_csv('data/home_loans.csv', low_memory=False) # read the csv file into a pandas dataframe object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what kind of data was collected, `pandas` has some handy commands:\n",
    "\n",
    "- `df.head()` will show us the first 5 rows of our dataset. You can also specify the first N rows, like `df.head(18)` will show us the first 18 rows.\n",
    "- `df.sample(10)` will show us 10 randomly sampled rows of our dataset\n",
    "- `df.shape` will tell us how many rows and how many columns are in the dataset\n",
    "- `df.columns` will list the names of all columns in the dataset\n",
    "- `df.describe()` will give you summary statistics about all numerical columns in the dataset\n",
    "\n",
    "### Question 1.A:  How many rows are in this dataset? How many columns?\n",
    "Rows: 369281, Columns: 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369281, 27)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.B: One of the columns in the dataset is the outcome value for each application, the value we will try to predict. Which column is that?\n",
    "loan_approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['town_name', 'county_name', 'loan_amount_000s', 'applicant_income_000s',\n",
       "       'property_type_name', 'occupied_by_owner', 'loan_type_name',\n",
       "       'is_hoepa_loan', 'loan_purpose_name', 'loan_approved',\n",
       "       'denial_reason_name_3', 'denial_reason_name_2', 'denial_reason_name_1',\n",
       "       'co_applicant_sex_name', 'co_applicant_race_name_5',\n",
       "       'co_applicant_race_name_4', 'co_applicant_race_name_3',\n",
       "       'co_applicant_race_name_2', 'co_applicant_race_name_1',\n",
       "       'co_applicant_ethnicity_name', 'applicant_sex_name',\n",
       "       'applicant_race_name_5', 'applicant_race_name_4',\n",
       "       'applicant_race_name_3', 'applicant_race_name_2',\n",
       "       'applicant_race_name_1', 'applicant_ethnicity_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.C: What reasons were given in this dataset for denying a loan application?\n",
    "Hint: There are 3 columns in the dataset that list why a loan was denied. Try looking up the pandas command to list the unique values in a column.\n",
    "\n",
    "Credit history, Insufficient cash (downpayment, closing costs), Employment history, Debt-to-income ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Other', 'Credit history',\n",
       "       'Insufficient cash (downpayment, closing costs)',\n",
       "       'Employment history', 'Debt-to-income ratio',\n",
       "       'Unverifiable information', 'Collateral',\n",
       "       'Credit application incomplete', 'Mortgage insurance denied'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.denial_reason_name_3.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.D: Given the denial reasons and the columns in this dataset, think about what information you _don't_ have about each application. Rank your top 3 _missing_ pieces of information about each application that could help you better predict the application's loan outcome.\n",
    "_Double click to write your answer question here. Show your work in code below if applicable._\n",
    "#1.  Credit History\n",
    "#2.  Employment History\n",
    "#3.  Asset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Data to Input to a Model\n",
    "Here we'll start using `scikit-learn` which provides simple library calls for most things we'd like to do in a simple machine learning pipeline. If you haven't used `scikit-learn` before this tutorial may be useful to give you a sense of what the library can do: https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "Machine learning models can only understand data that is represented numerically, but lots of the columns in our dataset like \"town_name\" are text _categorical_ data. Meanwhile, many models do better when continous numerical data is within small, consistent ranges, such as all data being between -1, 0 and 1, which is definitely not the case with our thousands of dollars loan units.\n",
    "\n",
    "So first, we will seperate out our samples (called _X_) into features we'd like to include in our model that are categorical or continous so that we can preprocess each appropriately seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_sex_name']\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s']\n",
    "\n",
    "X_cat = df[features_cat]\n",
    "X_num = df[features_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.A One Hot Encode Categorical Variables\n",
    "Run the following code to one hot encode the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_Home improvement</th>\n",
       "      <th>x0_Home purchase</th>\n",
       "      <th>x0_Refinancing</th>\n",
       "      <th>x1_Female</th>\n",
       "      <th>x1_Information not provided by applicant in mail, Internet, or telephone application</th>\n",
       "      <th>x1_Male</th>\n",
       "      <th>x1_Not applicable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_Home improvement  x0_Home purchase  x0_Refinancing  x1_Female  \\\n",
       "0                  0.0               0.0             1.0        1.0   \n",
       "1                  0.0               1.0             0.0        0.0   \n",
       "2                  0.0               0.0             1.0        0.0   \n",
       "3                  0.0               0.0             1.0        0.0   \n",
       "4                  1.0               0.0             0.0        1.0   \n",
       "\n",
       "   x1_Information not provided by applicant in mail, Internet, or telephone application  \\\n",
       "0                                                0.0                                      \n",
       "1                                                0.0                                      \n",
       "2                                                0.0                                      \n",
       "3                                                0.0                                      \n",
       "4                                                0.0                                      \n",
       "\n",
       "   x1_Male  x1_Not applicable  \n",
       "0      0.0                0.0  \n",
       "1      1.0                0.0  \n",
       "2      1.0                0.0  \n",
       "3      1.0                0.0  \n",
       "4      0.0                0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pandas.DataFrame(one_hot.toarray(), columns=enc.get_feature_names())\n",
    "X_cat_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.A: In your own words, how is one hot coding tranforming the categorical data? What does the term \"one-hot\" refer to?\n",
    "The term \"one-hot\" means that there is only one hot bit per feature.\n",
    "The one hot encoding transforms the categorical data into more machine understandable terms, in the sense that it makes more columns based on the unique values in the data and sets the bit to 1.0 only for that category which was specified for that particular row.\n",
    "For ex, for the first row(0) Refinancing and Female are a part of the original dataset and that's why their bits are set while the others are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.B Scaling down continuous numerical data\n",
    "Run the following code to normalize any continous numberical features, such as loan dollar amount, between -1 and 0. This process will ensure that the average of that feature, such as the average amount that a person asks for in loan amount, is scaled to 0. Values less than the average will be negative numbers, and values larger than the average will be positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amount_000s</th>\n",
       "      <th>applicant_income_000s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130864</td>\n",
       "      <td>0.016448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103680</td>\n",
       "      <td>-0.596232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101589</td>\n",
       "      <td>0.024727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128424</td>\n",
       "      <td>1.664059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266432</td>\n",
       "      <td>-0.000111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amount_000s  applicant_income_000s\n",
       "0         -0.130864               0.016448\n",
       "1         -0.103680              -0.596232\n",
       "2         -0.101589               0.024727\n",
       "3          0.128424               1.664059\n",
       "4          0.266432              -0.000111"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = preprocessing.scale(X_num)\n",
    "#type(scaled)\n",
    "#scaled.view()\n",
    "X_num_proc = pandas.DataFrame(scaled, columns=features_num)\n",
    "X_num_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.C Merge our feature sets into one sample dataset _X_ and fix NaN values\n",
    "Run the code below to combine the numerical and categorical feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amount_000s</th>\n",
       "      <th>applicant_income_000s</th>\n",
       "      <th>x0_Home improvement</th>\n",
       "      <th>x0_Home purchase</th>\n",
       "      <th>x0_Refinancing</th>\n",
       "      <th>x1_Female</th>\n",
       "      <th>x1_Information not provided by applicant in mail, Internet, or telephone application</th>\n",
       "      <th>x1_Male</th>\n",
       "      <th>x1_Not applicable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130864</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103680</td>\n",
       "      <td>-0.596232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101589</td>\n",
       "      <td>0.024727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.128424</td>\n",
       "      <td>1.664059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266432</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amount_000s  applicant_income_000s  x0_Home improvement  \\\n",
       "0         -0.130864               0.016448                  0.0   \n",
       "1         -0.103680              -0.596232                  0.0   \n",
       "2         -0.101589               0.024727                  0.0   \n",
       "3          0.128424               1.664059                  0.0   \n",
       "4          0.266432              -0.000111                  1.0   \n",
       "\n",
       "   x0_Home purchase  x0_Refinancing  x1_Female  \\\n",
       "0               0.0             1.0        1.0   \n",
       "1               1.0             0.0        0.0   \n",
       "2               0.0             1.0        0.0   \n",
       "3               0.0             1.0        0.0   \n",
       "4               0.0             0.0        1.0   \n",
       "\n",
       "   x1_Information not provided by applicant in mail, Internet, or telephone application  \\\n",
       "0                                                0.0                                      \n",
       "1                                                0.0                                      \n",
       "2                                                0.0                                      \n",
       "3                                                0.0                                      \n",
       "4                                                0.0                                      \n",
       "\n",
       "   x1_Male  x1_Not applicable  \n",
       "0      0.0                0.0  \n",
       "1      1.0                0.0  \n",
       "2      1.0                0.0  \n",
       "3      1.0                0.0  \n",
       "4      0.0                0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pandas.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.C The code line below removes any NaN values in our sample with 0. NaNs are missing values that a model won't be able to understand. What is the _semantic_ meaning of replaceing a NaN with 0 for the categorical variables? And for the continous numerical variables? \n",
    "If we replace NaN with 0's in categorical data(say loan_purpose_name), it would mean that none of the loan_purpose_name categories are set.\n",
    "If we replace NaN with 0's for continuous numerical data(say loan_amount_000s), it would mean that the loan_amount for that particular row was the average amount of loan amount of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(0) # remove NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.D Create our target array _y_ that our model will try to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['loan_approved'] # target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.E Split our data into training, test, and validation sets\n",
    "Run the code below to split the data. Both validation and test sets will be used for testing our model, but use the validation set while you are developing and improving your model, and leave the test for late stage evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258496, 9) (55392, 9) (55393, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.30) # split out into training 70% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.50) # split out into validation 15% of our data and test 15% of our data\n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # print data shape to check the sizing is correct\n",
    "#print(y_train.shape, y_validation.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.E:  In a  single sentence, what is the difference between train, test, and validation sets?\n",
    "The training set is utilised by the model to learn from the data set and the test data is used to check if the model has learnt correctly or not, while the validation data helps improve our model (you could do so by changing certain parameters to fit the better the prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Developing Models\n",
    "Scikit-learn has a substantial library of different models we can use for classification. Below are implemented two of the most simple classification models, Logistic Regression and Dummy Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    target_names = ['denied', 'approved']\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[7153 1902]\n",
      " [4491 4568]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      denied       0.61      0.79      0.69      9055\n",
      "    approved       0.71      0.50      0.59      9059\n",
      "\n",
      "    accuracy                           0.65     18114\n",
      "   macro avg       0.66      0.65      0.64     18114\n",
      "weighted avg       0.66      0.65      0.64     18114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs').fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred = model.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred) # finally evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dummy Classifier is a 'dummy' because it is going to use zero machine learning, and simply predict \"approve this loan\" (value 1) for every loan it sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[    0  9209]\n",
      " [    0 46183]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9209\n",
      "           1       0.83      1.00      0.91     46183\n",
      "\n",
      "    accuracy                           0.83     55392\n",
      "   macro avg       0.42      0.50      0.45     55392\n",
      "weighted avg       0.70      0.83      0.76     55392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "approve_everyone = DummyClassifier(strategy='constant', constant = 1).fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred_dummy = approve_everyone.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred_dummy) # finally evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.A: Considering only the data itself, why do Logistic Regression and the Dummy Classifier perform the same? What is the semantic meaning for why Dummy Classifier has such high accuracy?\n",
    "Almost all of the test set has it's loan_approved set to 1 which is why the dummy classifer performs so well. The number of approvals are way more than the rejections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Your turn!\n",
    "\n",
    "### Task 4.A: Create a new balanced dataset where exactly half of the samples are rejected loan applications and half are accepted loan application.\n",
    "_show your work below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['loan_approved'] == 1]\n",
    "df2 = df[df['loan_approved'] == 0]\n",
    "df1 = df1.truncate(after=60379)\n",
    "new_df = pandas.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.B: Below, retry training and evaluating a Logistic regression model on the updated data.\n",
    "_show your work below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84532, 9) (18114, 9) (18114, 9)\n",
      "Confusion matrix:\n",
      " [[7149 1965]\n",
      " [4547 4453]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      denied       0.61      0.78      0.69      9114\n",
      "    approved       0.69      0.49      0.58      9000\n",
      "\n",
      "    accuracy                           0.64     18114\n",
      "   macro avg       0.65      0.64      0.63     18114\n",
      "weighted avg       0.65      0.64      0.63     18114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas # import pandas library\n",
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pandas.read_csv('data/home_loans.csv', low_memory=False) # read the csv file into a pandas dataframe object\n",
    "\n",
    "df1 = df[df['loan_approved'] == 1]\n",
    "df2 = df[df['loan_approved'] == 0]\n",
    "df1 = df1.truncate(after=60379)\n",
    "df = pandas.concat([df1, df2])\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_sex_name']\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s']\n",
    "\n",
    "X_cat = df[features_cat]\n",
    "X_num = df[features_num]\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format\n",
    "\n",
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pandas.DataFrame(one_hot.toarray(), columns=enc.get_feature_names())\n",
    "X_cat_proc.head()\n",
    "\n",
    "scaled = preprocessing.scale(X_num)\n",
    "#type(scaled)\n",
    "#scaled.view()\n",
    "X_num_proc = pandas.DataFrame(scaled, columns=features_num)\n",
    "X_num_proc.head()\n",
    "\n",
    "X = pandas.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X.head()\n",
    "\n",
    "X = X.fillna(0) # remove NaN values\n",
    "y = df['loan_approved'] # target\n",
    "\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.30) # split out into training 70% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.50) # split out into validation 15% of our data and test 15% of our data\n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # print data shape to check the sizing is correct\n",
    "#print(y_train.shape, y_validation.shape, y_test.shape)\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    target_names = ['denied', 'approved']\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred, target_names = target_names))\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs').fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred = model.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred) # finally evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.C: Use your own imagination and experimentation to improve predictive performance for this task, modifying the model choices, feature choices, and data processing however you wish.\n",
    "_Important! Your ability to improve the model above the baseline after Task 4.B will count for 10% of this assignment grade, with 5% of that given for modest improvements to performance. Thus while we encourage you to experiment, do not sink excessive time into this task. We will test the performance on our own holdout dataset._\n",
    "\n",
    "_show your work below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 13) (18000, 13) (18000, 13)\n",
      "Confusion matrix:\n",
      " [[6984 2034]\n",
      " [4171 4811]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      denied       0.63      0.77      0.69      9018\n",
      "    approved       0.70      0.54      0.61      8982\n",
      "\n",
      "    accuracy                           0.66     18000\n",
      "   macro avg       0.66      0.66      0.65     18000\n",
      "weighted avg       0.66      0.66      0.65     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas # import pandas library\n",
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "df = pandas.read_csv('data/home_loans.csv', low_memory=False) # read the csv file into a pandas dataframe object\n",
    "\n",
    "#df.dropna(subset=['town_name'], inplace=True)\n",
    "\n",
    "df1 = df[df['loan_approved'] == 1]\n",
    "df2 = df[df['loan_approved'] == 0]\n",
    "\n",
    "df1 = df1.sample(60000)\n",
    "df2 = df2.sample(60000)\n",
    "df = pandas.concat([df1, df2])\n",
    "\n",
    "\n",
    "features_cat = ['loan_purpose_name', 'applicant_ethnicity_name', 'applicant_sex_name']\n",
    "features_num = ['loan_amount_000s', 'applicant_income_000s']\n",
    "\n",
    "X_cat = df[features_cat]\n",
    "X_num = df[features_num]\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format\n",
    "\n",
    "# Finally, put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "X_cat_proc = pandas.DataFrame(one_hot.toarray(), columns=enc.get_feature_names())\n",
    "X_cat_proc.head()\n",
    "\n",
    "scaled = preprocessing.scale(X_num)\n",
    "#type(scaled)\n",
    "#scaled.view()\n",
    "X_num_proc = pandas.DataFrame(scaled, columns=features_num)\n",
    "X_num_proc.head()\n",
    "\n",
    "X = pandas.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "X.head()\n",
    "\n",
    "X = X.fillna(0) # remove NaN values\n",
    "y = df['loan_approved'] # target\n",
    "\n",
    "X_train, X_TEMP, y_train, y_TEMP = train_test_split(X, y, test_size=0.30) # split out into training 70% of our data\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_TEMP, y_TEMP, test_size=0.50) # split out into validation 15% of our data and test 15% of our data\n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # print data shape to check the sizing is correct\n",
    "#print(y_train.shape, y_validation.shape, y_test.shape)\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    target_names = ['denied', 'approved']\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred, target_names = target_names))\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs').fit(X_train, y_train) # first fit (train) the model\n",
    "y_pred = model.predict(X_validation) # next get the model's predictions for a sample in the validation set\n",
    "metrics(y_validation, y_pred) # finally evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
